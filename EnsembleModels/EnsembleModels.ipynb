{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8gU7AYPXMmA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
    "\n",
    " **What you need to remember:**\n",
    "\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas\n",
    "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
    "\n",
    "Fill in your **NAME** and **AEM** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lO-jJrtNXMmH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"Ιωάννης Μπαρακλιλής\"\n",
    "AEM = \"3685\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh0EE7BJXMmJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_VpnGyWXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dQ9XoGQXMmK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JvHYIhS-XMmL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joKwpih2XMmM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJdwPr0bXMmM",
    "outputId": "70f064fa-7ee5-4985-f59d-9c94c1d1c3f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "('test_set_noclass.csv', <http.client.HTTPMessage at 0x7f7f8ce3b350>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t0OVtYr7XMmN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxOGHSmqXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure weighted and balanced accuracy of your models. You can use [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) and select both metrics to be measured during the evaluation. Otherwise, you can use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww_u4OlrXMmO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three **simple** estimators/classifiers. Test both soft and hard voting and choose the best one. Consider as simple estimators the following:\n",
    "\n",
    "\n",
    "*   Decision Trees\n",
    "*   Linear Models\n",
    "*   Probabilistic Models (Naive Bayes)\n",
    "*   KNN Models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwvPacgkXMmP",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "\n",
    "# Imports.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# To pick the individual models, I tested each one individually in the console and picked (and then tuned) the best performing 3. These results are shown in the results-comments of 3.0 (as similar tests had to be run to choose the base models).\n",
    "\n",
    "\"\"\" Tuning: Best combined models should lead to a good ensemble model.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'max_depth': [2, 5, 20, 50, 100, 200, None],\n",
    "    'max_leaf_nodes': [2, 5, 20, 50, 100, 200, None]\n",
    "}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=RANDOM_STATE), parameters, cv =10, verbose = 5, n_jobs = 6, scoring=['balanced_accuracy', 'f1_weighted'], refit = 'balanced_accuracy')\n",
    "clf.fit(X, y)\n",
    "cls = clf.best_estimator_\n",
    "print(cls)  # DecisionTreeClassifier(max_depth=20, max_leaf_nodes=50, random_state=RANDOM_STATE).\n",
    "scores = cross_validate(cls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=10, verbose=2)\n",
    "avg_fmeasure = np.average(scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.732930.\n",
    "avg_accuracy = np.average(scores[\"test_balanced_accuracy\"]) # The average accuracy -- Should be 0.720639.\n",
    "print(dict(avg_fmeasure=avg_fmeasure, avg_accuracy=avg_accuracy))\n",
    "\"\"\"\n",
    "cls1 = DecisionTreeClassifier(max_depth=20, max_leaf_nodes=50, random_state=RANDOM_STATE) # Classifier #1\n",
    "\n",
    "\"\"\" Tuning: Best combined models should lead to a good ensemble model.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'tol': [1e-2, 1e-3]\n",
    "}\n",
    "clf = GridSearchCV(LogisticRegression(solver = 'sag', random_state = RANDOM_STATE), parameters, cv =10, verbose = 5, n_jobs = 4, scoring=['balanced_accuracy', 'f1_weighted'], refit = 'balanced_accuracy')\n",
    "clf.fit(X, y)\n",
    "cls = clf.best_estimator_\n",
    "print(cls)  # LogisticRegression(random_state=RANDOM_STATE, solver='sag', tol=0.01).\n",
    "scores = cross_validate(cls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=4, verbose=2)\n",
    "avg_fmeasure = np.average(scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.851290.\n",
    "avg_accuracy = np.average(scores[\"test_balanced_accuracy\"]) # The average accuracy -- Should be 0.845610.\n",
    "print(dict(avg_fmeasure=avg_fmeasure, avg_accuracy=avg_accuracy))\n",
    "\"\"\"\n",
    "cls2 = LogisticRegression(random_state=RANDOM_STATE, solver='sag', tol=0.01) # Classifier #2\n",
    "\n",
    "\"\"\" Tuning: Best combined models should lead to a good ensemble model.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_neighbors': [1, 2, 3, 5, 10, 20, 50, 100],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "clf = GridSearchCV(KNeighborsClassifier(), parameters, cv =10, verbose = 5, n_jobs = 6, scoring=['balanced_accuracy', 'f1_weighted'], refit = 'balanced_accuracy')\n",
    "clf.fit(X, y)\n",
    "cls = clf.best_estimator_\n",
    "print(cls)  # KNeighborsClassifier(n_neighbors=10, weights='distance').\n",
    "scores = cross_validate(cls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=4, verbose=2)\n",
    "avg_fmeasure = np.average(scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.814493.\n",
    "avg_accuracy = np.average(scores[\"test_balanced_accuracy\"]) # The average accuracy -- Should be 0.807473.\n",
    "print(dict(avg_fmeasure=avg_fmeasure, avg_accuracy=avg_accuracy))\n",
    "\"\"\"\n",
    "cls3 = KNeighborsClassifier(n_neighbors=10, weights='distance') # Classifier #1\n",
    "\n",
    "soft_vcls = VotingClassifier([('dt', cls1), ('lr', cls2), ('knn', cls3)], voting=\"soft\") # Voting Classifier\n",
    "hard_vcls = VotingClassifier([('dt', cls1), ('lr', cls2), ('knn', cls3)], voting=\"hard\") # Voting Classifier\n",
    "\n",
    "svlcs_scores = cross_validate(soft_vcls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose = 2)\n",
    "s_avg_fmeasure = np.average(svlcs_scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.8431.\n",
    "s_avg_accuracy = np.average(svlcs_scores[\"test_balanced_accuracy\"]) # The average accuracy -- Should be 0.835.\n",
    "\n",
    "hvlcs_scores = cross_validate(hard_vcls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose = 2)\n",
    "h_avg_fmeasure = np.average(hvlcs_scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.8386.\n",
    "h_avg_accuracy = np.average(hvlcs_scores[\"test_balanced_accuracy\"]) # The average accuracy -- 0.8306.\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sQQvClrmXMmQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('dt',\n",
      "                              DecisionTreeClassifier(max_depth=20,\n",
      "                                                     max_leaf_nodes=50,\n",
      "                                                     random_state=42)),\n",
      "                             ('lr',\n",
      "                              LogisticRegression(random_state=42, solver='sag',\n",
      "                                                 tol=0.01)),\n",
      "                             ('knn',\n",
      "                              KNeighborsClassifier(n_neighbors=10,\n",
      "                                                   weights='distance'))],\n",
      "                 voting='soft')\n",
      "F1 Weighted-Score: 0.8431 & Balanced Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(soft_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(s_avg_fmeasure,4), round(s_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-iJK9pFaDka",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achive above 82% (Soft Voting Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XRNkVAvEYVbn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('dt',\n",
      "                              DecisionTreeClassifier(max_depth=20,\n",
      "                                                     max_leaf_nodes=50,\n",
      "                                                     random_state=42)),\n",
      "                             ('lr',\n",
      "                              LogisticRegression(random_state=42, solver='sag',\n",
      "                                                 tol=0.01)),\n",
      "                             ('knn',\n",
      "                              KNeighborsClassifier(n_neighbors=10,\n",
      "                                                   weights='distance'))])\n",
      "F1 Weighted-Score: 0.8386 & Balanced Accuracy: 0.8306\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(hard_vcls)\n",
    "print(\"F1 Weighted-Score: {} & Balanced Accuracy: {}\".format(round(h_avg_fmeasure,4), round(h_avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6M0CZO6aEHi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 80% in both! (Hard Voting Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVPuIxwFXMmR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two more complex estimators. Try different simple classifiers (like the ones mentioned before) for the combination of the initial estimators. Report your results in the following cell.\n",
    "\n",
    "Consider as complex estimators the following:\n",
    "\n",
    "*   Random Forest\n",
    "*   SVM\n",
    "*   Gradient Boosting\n",
    "*   MLP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX6T1qrFXMmS",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# To pick the individual models, I tested each one individually in the console and picked (and then tuned) the best performing 3. These results can be seen in the results of 3.0 (as similar tests had to be run to choose the base models).\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'tol': {}}\n",
    "for tol in [1e-3, 1e-2, 5e-2, 1e-1, 0.5]:\n",
    "    if tol not in all_metrics['tol']:\n",
    "        all_metrics['tol'][tol] = {'train': [], 'test': []}\n",
    "\n",
    "    clf = LinearSVC(tol=tol, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "    bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "    print(dict(tol=tol), end='\\n\\t\\t')\n",
    "    print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "    print(dict(f1_train=f1_score(y_train, clf.predict(X_train), average='weighted'),\n",
    "          f1_test=f1_score(y_test, clf.predict(X_test), average='weighted')))\n",
    "    all_metrics['tol'][tol]['train'].append(bal_acc_train)\n",
    "    all_metrics['tol'][tol]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['tol'].keys())\n",
    "ploty = [np.average(all_metrics['tol'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['tol'].keys())\n",
    "ploty = [np.average(all_metrics['tol'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for tol parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that all choices seem extremely close, so the model picked will be pretty much the same regardless of the specific parameter chosen.\n",
    "# So, after (hand) testing final ensembles of models, the submodel that had the best scores was the LinearSVC(tol=0.05, random_state=RANDOM_STATE), and so tol=0.05 was selected.\n",
    "\n",
    "clf = LinearSVC(tol=0.05, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be about 0.810540.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be about 0.804951.\n",
    "\"\"\"\n",
    "cls1 = LinearSVC(tol=5e-2, random_state=RANDOM_STATE) # Classifier #1\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'n_estimators': {}, 'learning_rate': {}}\n",
    "for n_estimators in [1, 10, 50, 100, 113, 150, 200, 300]:\n",
    "    if n_estimators not in all_metrics['n_estimators']:\n",
    "        all_metrics['n_estimators'][n_estimators] = {'train': [], 'test': []}\n",
    "    for learning_rate in [.1, .2, .5, .7, 1.]:\n",
    "        if learning_rate not in all_metrics['learning_rate']:\n",
    "            all_metrics['learning_rate'][learning_rate] = {'train': [], 'test': []}\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate, verbose = 0)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "        bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "        print(dict(n_estimators=n_estimators, learning_rate=learning_rate), end='\\n\\t\\t')\n",
    "        print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "        print(dict(f1_train=f1_score(y_train, clf.predict(X_train), average='weighted'),\n",
    "              f1_test=f1_score(y_test, clf.predict(X_test), average='weighted')))\n",
    "\n",
    "        all_metrics['n_estimators'][n_estimators]['train'].append(bal_acc_train)\n",
    "        all_metrics['n_estimators'][n_estimators]['test'].append(bal_acc_test)\n",
    "        all_metrics['learning_rate'][learning_rate]['train'].append(bal_acc_train)\n",
    "        all_metrics['learning_rate'][learning_rate]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for n_estimators parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plotx = sorted(all_metrics['learning_rate'].keys())\n",
    "ploty = [np.average(all_metrics['learning_rate'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['learning_rate'].keys())\n",
    "ploty = [np.average(all_metrics['learning_rate'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for learning_rate parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that, in average, the best parameters that optimize both train and test scores (balanced accuracy because it is the hardest to raise and it is observed that with high accuracy we have high f1 as well) is:\n",
    "#     * n_estimators = 113, after which both train and test scores have rise that does not justify the increased complexity.\n",
    "#     * learning_rate = 0.2, because the model overtrains and the testing score does not increase significantly.\n",
    "# As for validation:\n",
    "clf = GradientBoostingClassifier(n_estimators=113, learning_rate = 0.2, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be about 0.826294.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be about 0.816828.\n",
    "\"\"\"\n",
    "cls2 = GradientBoostingClassifier(n_estimators=113, learning_rate = 0.2, verbose = 0, random_state=RANDOM_STATE) # Classifier #2\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'hidden_layer_sizes': {}}\n",
    "for hidden_layer_sizes in [1, 10, 50, 100, 200, 300]:\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes] = {'train': [], 'test': []}\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "    bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "    bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "    f1_train = f1_score(y_train, clf.predict(X_train), average='weighted')\n",
    "    f1_test = f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    print(dict(hidden_layer_sizes=hidden_layer_sizes), end='\\n\\t\\t')\n",
    "    print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "    print(dict(f1_train=f1_train, f1_test=f1_test))\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes]['train'].append(bal_acc_train)\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['hidden_layer_sizes'].keys())\n",
    "ploty = [np.average(all_metrics['hidden_layer_sizes'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label='Train')\n",
    "plotx = sorted(all_metrics['hidden_layer_sizes'].keys())\n",
    "ploty = [np.average(all_metrics['hidden_layer_sizes'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label='Test')\n",
    "plt.title('Average balanced accuracy for hidden_layer_sizes parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that, in average, the best parameters that optimize both train and test scores (balanced accuracy because it is the hardest to raise and it is observed that with high accuracy we have high f1 as well) is:\n",
    "#     * hidden_layer_sizes = 200, after which train scores stay flat and test scores slowly drop.\n",
    "# As for validation:\n",
    "clf = MLPClassifier(hidden_layer_sizes = 200, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be about 0.858986.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be about 0.852655.\n",
    "\"\"\"\n",
    "cls3 = MLPClassifier(hidden_layer_sizes = 200, random_state=RANDOM_STATE) # Classifier #3 (Optional)\n",
    "\n",
    "# For the final estimator, I tried LogisticRegression, Decision Tree, Gaussian Naive Bayes and KNN with k=3. The best performing one was LogisticRegression, so that one was used in the end.\n",
    "scls = StackingClassifier(estimators=[(\"SVC\", cls1), (\"gbc\", cls2), (\"mlp\", cls3)], final_estimator=LogisticRegression(random_state=RANDOM_STATE), n_jobs=1) # Stacking Classifier\n",
    "scores = cross_validate(scls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "avg_fmeasure = np.average(scores[\"test_f1_weighted\"]) # The average f-measure -- Should be 0.8568.\n",
    "avg_accuracy = np.average(scores[\"test_balanced_accuracy\"]) # The average accuracy -- Should be 0.8501.\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-JLRzkQ1XMmT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(estimators=[('SVC', LinearSVC(random_state=42, tol=0.05)),\n",
      "                               ('gbc',\n",
      "                                GradientBoostingClassifier(learning_rate=0.2,\n",
      "                                                           n_estimators=113,\n",
      "                                                           random_state=42)),\n",
      "                               ('mlp',\n",
      "                                MLPClassifier(hidden_layer_sizes=200,\n",
      "                                              random_state=42))],\n",
      "                   final_estimator=LogisticRegression(random_state=42),\n",
      "                   n_jobs=1)\n",
      "F1 Weighted Score: 0.8568 & Balanced Accuracy: 0.8501\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1 Weighted Score: {} & Balanced Accuracy: {}\".format(round(avg_fmeasure,4), round(avg_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcgOx-HPvBI-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should achieve above 85% in both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-nqW51xXMmU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPG8MdFLXMmV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1_weighted/balanced_accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmkaP-DjXMmV",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\"\"\" Hyper-parameter tuning process:\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "ns = []\n",
    "baccs = []\n",
    "f1s = []\n",
    "# Trying out different n_estimators.\n",
    "for i in list(range(20, 1001, 100)): # Then list(range(100, 201, 50)), then list(range(140, 161, 5)). Each new range was tested after the result of the previous ones (focusing on the areas of great scores).\n",
    "    clf = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=i, random_state=RANDOM_STATE,\n",
    "                      n_jobs=6)\n",
    "    clf.fit(X_train, y_train)\n",
    "    ns.append(i)\n",
    "    baccs.append(balanced_accuracy_score(y_test, clf.predict(X_test)))\n",
    "    f1s.append(f1_score(y_test, clf.predict(X_test), average=\"weighted\"))\n",
    "    print('%s done: balanced_accuracy = %f, f1_score = %f'%(i, baccs[-1], f1s[-1]))\n",
    "# Plot results.\n",
    "plt.plot(ns, baccs, 'b', label = 'Balanced accuracy.'); plt.plot(ns, f1s, 'r', label = 'F1 weighted.'); plt.legend(); plt.show()\n",
    "\n",
    "# The best number of estimators looked like 155: BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=155, random_state=RANDOM_STATE, n_jobs=6)\n",
    "# Scores (computed below for all classifier ensembles): F1 Weighted = 0.8062, Balanced Accuracy = 0.792.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "ens1 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=155, random_state=RANDOM_STATE, n_jobs=6) # Bagging with default trees (155 estimators because with more, the scores wouldn't get any better).\n",
    "\n",
    "\"\"\" Hyper-parameter tuning process:\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "params = {\n",
    "    'n_estimators': [300, 350, 400, 450, 500],  # At first attempt (before refinement): [10, 50, 100, 200, 300, 400].\n",
    "    'max_samples': [.55, .58, .6, .65, .68],  # At first attempt (before refinement): [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0].\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE, n_jobs=6), params, n_jobs = 1, cv = 3, verbose = 5, scoring=['balanced_accuracy', 'f1_weighted'], refit = 'balanced_accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_estimator_)  # BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), max_samples=0.68, n_estimators=350, n_jobs=6, random_state=RANDOM_STATE).\n",
    "print('Best balanced accuracy: %f' % clf.best_score)\n",
    "\n",
    "# Classifier scores (computed below for all classifier ensembles): F1 Weighted = 0.8114, Balanced Accuracy = 0.7975.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "ens2 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "                         max_samples=0.68, n_estimators=350, n_jobs=6,\n",
    "                         random_state=RANDOM_STATE) # Pasting: 68% of samples used.\n",
    "\n",
    "\n",
    "\"\"\" Hyper-parameter tuning process:\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "models=[]\n",
    "metricss=[]\n",
    "max_metrics = (-1, -1)\n",
    "max_model = None\n",
    "for n_estimators in [100, 200, 300]:\n",
    "    for max_samples in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "        for max_features in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "            clf = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), random_state=RANDOM_STATE, n_jobs=6, n_estimators=n_estimators\n",
    "                                    , max_samples = max_samples, max_features = max_features)\n",
    "            clf.fit(X_train, y_train)\n",
    "            metrics = balanced_accuracy_score(y_test, clf.predict(X_test)), f1_score(y_test, clf.predict(X_test), average = \"weighted\")\n",
    "            # Find max model (and update variable). Priority is given to balanced accuracy because it tends to be lower than f1.\n",
    "            models.append(clf), metricss.append(metrics)\n",
    "            if metrics[0] >= max_metrics[0] and metrics[1] > max_metrics[1]:\n",
    "                max_metrics = metrics\n",
    "                max_model = clf\n",
    "            print({'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}, metrics)\n",
    "print(max_model)\n",
    "\n",
    "# The best number of estimators looked like max_features=0.3, max_samples=0.9, n_estimators=200 :\n",
    "#                       BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), max_features=0.3, max_samples=0.9, n_estimators=200, n_jobs=6, random_state=RANDOM_STATE).\n",
    "# Scores (computed below for all classifier ensembles): F1 Weighted = 0.8149, Balanced Accuracy = 0.8008.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "ens3 = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "                         max_features=0.3, max_samples=0.9, n_estimators=200, n_jobs=6,\n",
    "                         random_state=RANDOM_STATE) # Random Patches: 90% of samples and 30% of features used.\n",
    "\n",
    "\"\"\" Hyper-parameter tuning process:\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "params = {\n",
    "'criterion': [\"gini\", \"entropy\"],\n",
    "'max_depth': [2, 10, 25, 50, 70, 100, 250, 500, 600, 700, 800, 1000, None],\n",
    "'max_features': [None] + list(range(2, 4096, 100)),\n",
    "'min_samples_leaf': [1, 5, 7, 10, 30, 40, 50, 70, 80, 100, 200],\n",
    "'min_samples_split': [2, 5, 7, 10, 30, 40, 50, 70, 80, 100, 200],\n",
    "'max_leaf_nodes': [2, 10, 50, 100, 200, 300, 500, 700, 900, 1000, 1100, 1200, 1350, 1500, 1700, 2000, 3000, 5000, None]\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = 5, n_iter = 500, cv = 5, verbose = 1, scoring=['balanced_accuracy', 'f1_weighted'], refit = 'balanced_accuracy')\n",
    "# No random state because I tried running it a number of times until it resulted in a passable model.\n",
    "clf.fit(X, y)\n",
    "print(clf.best_estimator_)  # DecisionTreeClassifier(criterion='entropy', max_depth=25, max_features=1102, max_leaf_nodes=900, min_samples_leaf=50, random_state=RANDOM_STATE).\n",
    "print('bacc =', clf.best_score_)\n",
    "\n",
    "# Scores (computed below for all classifier ensembles): F1 Weighted = 0.7267, Balanced Accuracy = 0.7148.\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=25, max_features=1102, max_leaf_nodes=900, min_samples_leaf=50, random_state=RANDOM_STATE) # Simple tree classifier with optimized parameters.\n",
    "\n",
    "ens1_scores = cross_validate(ens1, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "ens2_scores = cross_validate(ens2, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "ens3_scores = cross_validate(ens3, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "tree_scores = cross_validate(tree, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "\n",
    "\n",
    "f_measures = dict()\n",
    "accuracies = dict()\n",
    "# Example f_measures = {'Simple Decision': 0.8551, 'Ensemble with random ...': 0.92, ...}\n",
    "f_measures = {'Simple Decision': np.average(tree_scores['test_f1_weighted']), 'Plain Bagging': np.average(ens1_scores['test_f1_weighted']), 'Pasting': np.average(ens2_scores['test_f1_weighted']),\n",
    "              'Random Patches': np.average(ens3_scores['test_f1_weighted'])}\n",
    "             # Should be {'Simple Decision': 0.7267, 'Plain Bagging': 0.8062, 'Pasting': 0.8114, 'Random Patches': 0.8149}\n",
    "accuracies = {'Simple Decision': np.average(tree_scores['test_balanced_accuracy']), 'Plain Bagging': np.average(ens1_scores['test_balanced_accuracy']), 'Pasting': np.average(ens2_scores['test_balanced_accuracy']),\n",
    "              'Random Patches': np.average(ens3_scores['test_balanced_accuracy'])}\n",
    "             # Should be {'Simple Decision': 0.7148, 'Plain Bagging': 0.792, 'Pasting': 0.7975, 'Random Patches': 0.8008}\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IUqhDUuCXMmW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                  n_estimators=155, n_jobs=6, random_state=42)\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                  max_samples=0.68, n_estimators=350, n_jobs=6,\n",
      "                  random_state=42)\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                  max_features=0.3, max_samples=0.9, n_estimators=200, n_jobs=6,\n",
      "                  random_state=42)\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=25, max_features=1102,\n",
      "                       max_leaf_nodes=900, min_samples_leaf=50,\n",
      "                       random_state=42)\n",
      "Classifier:Simple Decision -  F1 Weighted:0.7267\n",
      "Classifier:Plain Bagging -  F1 Weighted:0.8062\n",
      "Classifier:Pasting -  F1 Weighted:0.8114\n",
      "Classifier:Random Patches -  F1 Weighted:0.8149\n",
      "Classifier:Simple Decision -  BalancedAccuracy:0.7148\n",
      "Classifier:Plain Bagging -  BalancedAccuracy:0.792\n",
      "Classifier:Pasting -  BalancedAccuracy:0.7975\n",
      "Classifier:Random Patches -  BalancedAccuracy:0.8008\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1 Weighted:{}\".format(name,round(score,4)))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  BalancedAccuracy:{}\".format(name,round(score,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqdXTE_2XMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU9POFftXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "***\n",
    "There are four classifiers tuned by using either gridsearch or holdout:\n",
    "1. **Simple Decision**: A decision tree classifier which has been optimized (having the optimal hyperparameters after tuning). It cannot model the data as well as the ensemble models and has the worse scores in all the metrics (F1 Weighted:0.7267, BalancedAccuracy:0.7148).\n",
    "2. **Plain Bagging**: A bagging ensemble having a plain (with no parameters deviating from the default) tree as a base model which has been tuned to maximize the two metrics. The number of estimators is 155, because any additional wouldn't result in (significantly) better scores. Its scores are much better than the simple decision tree (F1 Weighted:0.8062, BalancedAccuracy:0.792), meaning it overcame some of its problems.\n",
    "3. **Pasting**: A bagging ensemble having a plain (with no parameters deviating from the default) tree as a base model which uses pasting having every base model use 68% of the training data given and number of estimators = 350 (these numbers were calculated during tuning). We can see that it was more successful than the plain tree having better scores than it (F1 Weighted:0.8114, BalancedAccuracy:0.7975), and slight better than plain bagging.\n",
    "4. **Random Patches**: A bagging ensemble having a plain (with no parameters deviating from the default) tree as a base model which uses random patches having every base model use 90% of the training data and 30% of the features (of the data) given and number of estimators = 200 (these numbers were calculated during tuning). We can see that it was the most successful model overall (F1 Weighted:0.8149, BalancedAccuracy:0.8008) but close enough to the pasting ensemble to be considered its equivalent.\n",
    "\n",
    "From the results above, it can be observed that **all the ensembles have better results** than the plain (base) decision tree.\n",
    "\n",
    "If we were to pick one of these models (as the best one), it would be the random patches classifier due to the fact that it appears to have a (very) slightly better score than its counterparts.\n",
    "However, since the difference is so slight, it can be argued that the tree ensemble models are equivalent and picking any one of them would be about the same, but still much better than the ordinary decision tree.\n",
    "\n",
    "Also, we can observe that the tree ensembles were not as successful as the other ensembles constructed in 1.1 and 1.2, since even with a lot of classifiers and tuning they (in the best case) reached about 81% F1 score and 80% balanced accuracy while the others (the ones in 1.1, 1.2) reached scores above 82% in both metrics with a much smaller number of classifiers.\n",
    "\n",
    "(Note: For some reason I cannot find a solution to, each time the notebook is run, different results arise, even though there should be the fixed random state in all models.\n",
    "So, I record all my findings the last time run (you should be able to see the saved output of the cell above) and my conclusions.\n",
    "Also, it should also be noted that after each run the above conclusions can be still made: All the ensembles are always better than the plain tree and have about the same scores.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkJeuV1FXMmX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApNEPcWEXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "***\n",
    "\n",
    "#### About the bagging classifiers\n",
    "It is true that increasing the number of estimators in a bagging classifier will result in higher a training time.\n",
    "\n",
    "1. Using parallelism.\n",
    "  *   The simplest way to combat this is to use multiple threads and cores (the **n_jobs** parameter in sklearn) and as many possible ideally (-1 to use) all available CPU threads, so that many models can be trained in parallel and reduce the total time spent training.\n",
    "\n",
    "  *   However, even though increasing the n_jobs parameter does reduce the ensemble's total training time, it **is highly dependent on the computer's CPU capabilities**, available threads and current workload (among other things) and  may not work for every situation. But, if it is available to us and is effective for the problem at hand, it is the best choice around (it speeds up training without compromising it).\n",
    "\n",
    "2. Using a subset of the given data.\n",
    "  *   Alternatively, the number of samples used in each model's training can be reduced (**max_samples** parameter in sklearn can be used (change the default 1.0 to a lower number) to reduce the number of training examples each sub-model uses in training). This should reduce the burden of training each model and, as the result, reduce the ensemble's overall training time.\n",
    "  *   Similarly, the number of features used in each model's training can be reduced, (the max_features in sklearn (change the default 1.0 to a lower number)), expecting similar results.\n",
    "  *   However, choosing to do one (or both) of the two above options, changes the set of training data given to each sub-model, having the **risk of changing the model's results** or even making the ensemble model as a whole less accurate since some important information has a higher chance of getting lost in the (re)sampling.\n",
    "3. Picking the best number of estimators.\n",
    "\n",
    "    Lastly, infinitely increasing the number of estimators won't result into an infinitely \"better\" model since its metrics will, eventually, reach a plateau and won't increase or maybe will even decrease.\n",
    "    So, we can set the number of estimators to the one that maximizes the important metrics after searching for it, either by hand (or by graphing the results) or using a more methodic approach such as GridSearchCV for a wide range of estimator numbers.\n",
    "\n",
    "    Even though this solution can reduce training time, it cannot solve the underlying problem of the time needing to train each sub-model, especially if the \"best\" number of estimators is high.\n",
    "\n",
    "\n",
    "\n",
    "#### About the boosting classifiers\n",
    "Mirroring the solutions above, about the solutions for training time increase caused by increasing the number of estimators in a boosting classifier:\n",
    "1. Using parallelism.\n",
    "  *   Unfortunately, due to the nature of the boosting classifiers requiring one model having the results of the previous model's training, parallelism is **impossible** for these kinds of classifiers.\n",
    "2. Using a subset of the given data.\n",
    "  *   This too is **not possible** (in the same way as in bagging classifiers), because each sub-model learns from the previous one's errors, so no data can be missing.\n",
    "3. Picking the best number of estimators.\n",
    "  *   Fortunately, this solution **can be used**, but as stated above, it cannot solve the problem entirely, but it can help somewhat.\n",
    "  *   Additionally, if available, the **learning rate training parameter** can be increased, so that the classifier reaches a low total error more quickly and in that case the number of estimators can be decreased to accommodate for the increased learning speed (since it lowers the error faster, it needs fewer steps to take and thus, a lower number of estimators).\n",
    "    But, this too isn't always the best choice since it might compromise the learning process and result in a worse model.\n",
    "\n",
    "However, there as boosting classifiers such as the Gradient Boosting Classifier that can use approximation (like XGBClassifier, for example) to speed up the training process. This may potentially yield worse results than the normal training process but is much quicker because of the approximation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgvsCbUGXMmY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6daX2mRXMmZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure (weighted) & balanced accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve a balanced accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00xAQ0HfXMmZ",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)\n",
    "all_metrics = {'n_estimators':{}, 'max_features': {}, 'max_samples': {}}\n",
    "for n_estimators in [1, 10, 25, 50, 60]:\n",
    "    all_metrics['n_estimators'][n_estimators] = {'train': [], 'test': []}\n",
    "    for max_features in [.2, .4, .7, .9, 1.]:\n",
    "        if max_features not in all_metrics['max_features']:\n",
    "            all_metrics['max_features'][max_features] = {'train': [], 'test': []}\n",
    "        for max_samples in [.5, .7, .85, .9, 1.]:\n",
    "            if max_samples not in all_metrics['max_samples']:\n",
    "                all_metrics['max_samples'][max_samples] = {'train': [], 'test': []}\n",
    "\n",
    "            clf = BaggingClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE, solver='sag', tol=0.05),\n",
    "                     max_features=max_features, max_samples=max_samples, n_estimators=n_estimators, n_jobs=4, random_state=RANDOM_STATE)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "            bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "            print(dict(max_features=max_features, max_samples=max_samples, n_estimators=n_estimators), end='\\n\\t\\t')\n",
    "            print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "            print(dict(f1_train=f1_score(y_train, clf.predict(X_train), average='weighted'),\n",
    "                  f1_test=f1_score(y_test, clf.predict(X_test), average='weighted')))\n",
    "            all_metrics['n_estimators'][n_estimators]['train'].append(bal_acc_train)\n",
    "            all_metrics['n_estimators'][n_estimators]['test'].append(bal_acc_test)\n",
    "            all_metrics['max_features'][max_features]['train'].append(bal_acc_train)\n",
    "            all_metrics['max_features'][max_features]['test'].append(bal_acc_test)\n",
    "            all_metrics['max_samples'][max_samples]['train'].append(bal_acc_train)\n",
    "            all_metrics['max_samples'][max_samples]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "\n",
    "plt.title('Average balanced accuracy for n_estimators parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotx = sorted(all_metrics['max_features'].keys())\n",
    "ploty = [np.average(all_metrics['max_features'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "\n",
    "plotx = sorted(all_metrics['max_features'].keys())\n",
    "ploty = [np.average(all_metrics['max_features'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "\n",
    "plt.title('Average balanced accuracy for max_features parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plotx = sorted(all_metrics['max_samples'].keys())\n",
    "ploty = [np.average(all_metrics['max_samples'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "\n",
    "plotx = sorted(all_metrics['max_samples'].keys())\n",
    "ploty = [np.average(all_metrics['max_samples'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "\n",
    "plt.title('Average balanced accuracy for max_samples parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that, in average, the best parameters that optimize both train and test scores (balanced accuracy because it is the hardest to raise and it is observed that with high accuracy we have high f1 as well) are:\n",
    "#     * n_estimators = 25, which is a local peak, after which train scores don't increase significantly and test slowly fall off.\n",
    "#     * max_features = 0.7, since it is near the peak and after (hand) testing final ensembles of models, the submodel that had the best scores had this parameter.\n",
    "#     * max_samples = 0.85 since it is near the peak and after (hand) testing final ensembles of models, the submodel that had the best scores had this parameter.\n",
    "\n",
    "# As for validation:\n",
    "clf = BaggingClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE, solver='sag', tol=0.05),\n",
    "                        max_features=0.7, max_samples=0.85, n_estimators=25, n_jobs=6, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be ~ 0.847500.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be ~ 0.840846.\n",
    "\"\"\"\n",
    "clf1 = BaggingClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE, solver='sag', tol=0.05), max_features=0.7, max_samples=0.85, n_estimators=25, n_jobs=6, random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'tol': {}}\n",
    "for tol in [1e-3, 1e-2, 5e-2, 1e-1, 0.5]:\n",
    "    if tol not in all_metrics['tol']:\n",
    "        all_metrics['tol'][tol] = {'train': [], 'test': []}\n",
    "\n",
    "    clf = LinearSVC(tol=tol, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "    bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "    print(dict(tol=tol), end='\\n\\t\\t')\n",
    "    print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "    print(dict(f1_train=f1_score(y_train, clf.predict(X_train), average='weighted'),\n",
    "          f1_test=f1_score(y_test, clf.predict(X_test), average='weighted')))\n",
    "    all_metrics['tol'][tol]['train'].append(bal_acc_train)\n",
    "    all_metrics['tol'][tol]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['tol'].keys())\n",
    "ploty = [np.average(all_metrics['tol'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['tol'].keys())\n",
    "ploty = [np.average(all_metrics['tol'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for tol parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that all choices seem extremely close, so (since it is a simple model compared to others) the model choice will be pretty much the same.\n",
    "# So, after (hand) testing final ensembles of models, the submodel that had the best scores was the LinearSVC(tol=0.05, random_state=RANDOM_STATE), and so tol=0.05 was selected.\n",
    "\n",
    "clf = LinearSVC(tol=0.05, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be ~ 0.810540.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be ~ 0.804951.\n",
    "\"\"\"\n",
    "clf2 = LinearSVC(tol=0.05, random_state=RANDOM_STATE)\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'n_estimators': {}, 'learning_rate': {}}\n",
    "for n_estimators in [1, 10, 50, 100, 113, 150, 200, 300]:\n",
    "    if n_estimators not in all_metrics['n_estimators']:\n",
    "        all_metrics['n_estimators'][n_estimators] = {'train': [], 'test': []}\n",
    "    for learning_rate in [.1, .2, .5, .7, 1.]:\n",
    "        if learning_rate not in all_metrics['learning_rate']:\n",
    "            all_metrics['learning_rate'][learning_rate] = {'train': [], 'test': []}\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate = learning_rate, verbose = 0)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "        bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "        print(dict(n_estimators=n_estimators, learning_rate=learning_rate), end='\\n\\t\\t')\n",
    "        print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "        print(dict(f1_train=f1_score(y_train, clf.predict(X_train), average='weighted'),\n",
    "              f1_test=f1_score(y_test, clf.predict(X_test), average='weighted')))\n",
    "\n",
    "        all_metrics['n_estimators'][n_estimators]['train'].append(bal_acc_train)\n",
    "        all_metrics['n_estimators'][n_estimators]['test'].append(bal_acc_test)\n",
    "        all_metrics['learning_rate'][learning_rate]['train'].append(bal_acc_train)\n",
    "        all_metrics['learning_rate'][learning_rate]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['n_estimators'].keys())\n",
    "ploty = [np.average(all_metrics['n_estimators'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for n_estimators parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plotx = sorted(all_metrics['learning_rate'].keys())\n",
    "ploty = [np.average(all_metrics['learning_rate'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label = 'Train')\n",
    "plotx = sorted(all_metrics['learning_rate'].keys())\n",
    "ploty = [np.average(all_metrics['learning_rate'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label = 'Test')\n",
    "plt.title('Average balanced accuracy for learning_rate parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that, in average, the best parameters that optimize both train and test scores (balanced accuracy because it is the hardest to raise and it is observed that with high accuracy we have high f1 as well) are:\n",
    "#     * n_estimators = 113, after which both train and test scores have a rise that does not justify the increased complexity.\n",
    "#     * learning_rate = 0.2, because the model overtrains and the testing score does not increase significantly.\n",
    "# As for validation:\n",
    "clf = GradientBoostingClassifier(n_estimators=113, learning_rate = 0.2, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be ~ 0.829364.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be ~ 0.819656.\n",
    "\"\"\"\n",
    "clf3 = GradientBoostingClassifier(n_estimators=113, learning_rate = 0.2, verbose = 0, random_state=RANDOM_STATE) # Classifier #2\n",
    "\n",
    "\"\"\" Tuning:\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "all_metrics = {'hidden_layer_sizes': {}}\n",
    "for hidden_layer_sizes in [1, 10, 50, 100, 200, 300]:\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes] = {'train': [], 'test': []}\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "    bal_acc_train = balanced_accuracy_score(y_train, clf.predict(X_train))\n",
    "    bal_acc_test = balanced_accuracy_score(y_test, clf.predict(X_test))\n",
    "    f1_train = f1_score(y_train, clf.predict(X_train), average='weighted')\n",
    "    f1_test = f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    print(dict(hidden_layer_sizes=hidden_layer_sizes), end='\\n\\t\\t')\n",
    "    print(dict(bal_acc_train=bal_acc_train, bal_acc_test=bal_acc_test), end='\\n\\t\\t')\n",
    "    print(dict(f1_train=f1_train, f1_test=f1_test))\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes]['train'].append(bal_acc_train)\n",
    "    all_metrics['hidden_layer_sizes'][hidden_layer_sizes]['test'].append(bal_acc_test)\n",
    "\n",
    "# Plot to arrive to conclusions.\n",
    "plotx = sorted(all_metrics['hidden_layer_sizes'].keys())\n",
    "ploty = [np.average(all_metrics['hidden_layer_sizes'][i]['train']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'r', label='Train')\n",
    "plotx = sorted(all_metrics['hidden_layer_sizes'].keys())\n",
    "ploty = [np.average(all_metrics['hidden_layer_sizes'][i]['test']) for i in plotx]\n",
    "plt.plot(plotx, ploty, 'g', label='Test')\n",
    "plt.title('Average balanced accuracy for hidden_layer_sizes parameter')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- After we see the graphs, it can be observed that, in average, the best parameters that optimize both train and test scores (balanced accuracy because it is the hardest to raise and it is observed that with high accuracy we have high f1 as well) is:\n",
    "#     * hidden_layer_sizes = 200, after which train scores stay flat and test scores slowly drop.\n",
    "# As for validation:\n",
    "clf = MLPClassifier(hidden_layer_sizes = 200, random_state=RANDOM_STATE)\n",
    "scores = cross_validate(clf, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "print('avg_fmeasure =', np.average(scores[\"test_f1_weighted\"]) )# The average f-measure -- Should be ~ 0.858986.\n",
    "print('avg_accuracy =', np.average(scores[\"test_balanced_accuracy\"])) # The average accuracy -- Should be ~ 0.852655.\n",
    "\"\"\"\n",
    "clf4 = MLPClassifier(hidden_layer_sizes = 200, random_state=RANDOM_STATE)\n",
    "\n",
    "\"\"\"\n",
    "# Trying out different ensemble combinations to form a new ensemble.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "hard_vcls = VotingClassifier([('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)], voting=\"hard\") # Hard voting Classifier.\n",
    "scls_lr = StackingClassifier(estimators=[('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)],\n",
    "                             final_estimator=LogisticRegression(random_state=RANDOM_STATE), n_jobs=1) # Stacking Classifier.\n",
    "scls_gnb = StackingClassifier(estimators=[('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)],\n",
    "                             final_estimator=GaussianNB(), n_jobs=1) # Stacking Classifier.\n",
    "scls_knn = StackingClassifier(estimators=[('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)],\n",
    "                             final_estimator=KNeighborsClassifier(n_neighbors=5), n_jobs=1) # Stacking Classifier.\n",
    "scls_dt = StackingClassifier(estimators=[('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)],\n",
    "                             final_estimator=DecisionTreeClassifier(random_state=RANDOM_STATE), n_jobs=1) # Stacking Classifier.\n",
    "\n",
    "# Pick the model with the maximum balanced accuracy (since usually (in the tuning tests above) it is lower than f1 and harder to raise while f1 can usually be raised to a good enough point).\n",
    "# Also, results will be printed to verify the above and ensure a good model.\n",
    "\n",
    "validate_best_accuracy = -1  # Initial (non-achievable) low max.\n",
    "validate_best_cls = hard_vcls\n",
    "\n",
    "clf_ens = hard_vcls.fit(X_train, y_train)\n",
    "ba = balanced_accuracy_score(y_test, clf_ens.predict(X_test))\n",
    "print({'balanced_accuracy': ba, 'f1_weighted': f1_score(y_test, clf_ens.predict(X_test), average=\"weighted\")})\n",
    "if ba > validate_best_accuracy:\n",
    "    validate_best_accuracy = ba\n",
    "    validate_best_cls = hard_vcls\n",
    "\n",
    "clf_ens = scls_lr.fit(X_train, y_train)\n",
    "ba = balanced_accuracy_score(y_test, clf_ens.predict(X_test))\n",
    "print({'balanced_accuracy': ba, 'f1_weighted': f1_score(y_test, clf_ens.predict(X_test), average=\"weighted\")})\n",
    "if ba > validate_best_accuracy:\n",
    "    validate_best_accuracy = ba\n",
    "    validate_best_cls = scls_lr\n",
    "\n",
    "clf_ens = scls_gnb.fit(X_train, y_train)\n",
    "ba = balanced_accuracy_score(y_test, clf_ens.predict(X_test))\n",
    "print({'balanced_accuracy': ba, 'f1_weighted': f1_score(y_test, clf_ens.predict(X_test), average=\"weighted\")})\n",
    "if ba > validate_best_accuracy:\n",
    "    validate_best_accuracy = ba\n",
    "    validate_best_cls = scls_gnb\n",
    "\n",
    "clf_ens = scls_knn.fit(X_train, y_train)\n",
    "ba = balanced_accuracy_score(y_test, clf_ens.predict(X_test))\n",
    "print({'balanced_accuracy': ba, 'f1_weighted': f1_score(y_test, clf_ens.predict(X_test), average=\"weighted\")})\n",
    "if ba > validate_best_accuracy:\n",
    "    validate_best_accuracy = ba\n",
    "    validate_best_cls = scls_knn\n",
    "\n",
    "clf_ens = scls_dt.fit(X_train, y_train)\n",
    "ba = balanced_accuracy_score(y_test, clf_ens.predict(X_test))\n",
    "print({'balanced_accuracy': ba, 'f1_weighted': f1_score(y_test, clf_ens.predict(X_test), average=\"weighted\")})\n",
    "if ba > validate_best_accuracy:\n",
    "    validate_best_accuracy = ba\n",
    "    validate_best_cls = scls_dt\n",
    "\n",
    "best_cls = validate_best_cls\n",
    "print(best_cls)\n",
    "\n",
    "# The best classifier was hard_vcls.\n",
    "\"\"\"\n",
    "\n",
    "# Best clf is the one that had the best balanced accuracy score.\n",
    "best_cls = VotingClassifier([('bagging_logistic', clf1), ('svc', clf2), ('grad_boosting', clf3), ('mlp', clf4)], voting=\"hard\") # Hard voting Classifier.\n",
    "\n",
    "print(best_cls)\n",
    "\n",
    "scores = cross_validate(best_cls, X, y, cv = 10, scoring=[\"f1_weighted\", \"balanced_accuracy\"], n_jobs=1, verbose=5)\n",
    "best_fmeasure = np.average(scores['test_f1_weighted'])  # Should be ~ 0.8575316429138551.\n",
    "best_accuracy = np.average(scores['test_balanced_accuracy'])  # Should be ~ 0.8550719461773506.\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FbLB09agXMma",
    "outputId": "be9abfc9-aa0d-4e22-9f07-46926fd1d987",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "F1 Weighted-Score:0.8575316429138551 & Balanced Accuracy:0.8550719461773506\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "#print(best_cls)\n",
    "print(\"F1 Weighted-Score:{} & Balanced Accuracy:{}\".format(best_fmeasure, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnos1uqzXMma",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5dAfbTfXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "***\n",
    "### **Process**\n",
    "1. Firstly, I tried (tested interactively in the console (some of these tests are recorded below) and with default hyperparameters) normal and bagged \"versions\" of all the classifiers (both simple and complex) mentioned in this notebook (specifically, in \"1.0 Testing different ensemble methods\").\n",
    "2. From these tests, I chose some well performing models and tuned them (you can see the tuning process in the multiline comments above each classifier as well as the results and their explanations) to be as accurate as possible (as good base models should lead to good ensembles).\n",
    "3. Then, these were combined in different ways as an ensemble (this can be seen in the cell above where the tests for different model mixing ways (hard voting, stacking with logistic regression, etc.).\n",
    "4. After that, after reaching a good result (>85% but still a little lower than 1.2, having that as the baseline), I tried slightly different well-performing models as base models and/or tweaking some hyperparameters of present base models (base on the results on their individual tuning) trying to get as good final scores as possible (and of course to get something better than 1.2 results). This is because it was observed that the absolute best performing models and best tuning of individual (base) models of the ensemble didn't necessarily translate to the best ensemble.\n",
    "5. Finally, after already having a good model and not reaching a better one with the tests above, I stick with the best achieved so far.\n",
    "6. In conclusion the model was not too much better than the one in 1.2 but has a (slightly) better Balanced Accuracy and about the same f1 score.\n",
    "\n",
    "We can see that a balanced accuracy > 84% was achieved as well as F1 weighted > 85%.\n",
    "\n",
    "### **Results of attempted base / simple ensemble classifiers (scores obtained with 10-fold validation).**\n",
    "* `DecisionTreeClassifier(random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.6863592882008598 & Balanced Accuracy:0.6774439103601141\n",
    "* `LogisticRegression(random_state=42, solver='sag', tol=0.05)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.852717589259744 & Balanced Accuracy:0.8463461294771439\n",
    "* `KNeighborsClassifier()`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8037038987819196 & Balanced Accuracy:0.7962067223144675\n",
    "* `GaussianNB()`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.6880036235582245 & Balanced Accuracy:0.6684547863432078\n",
    "* `LinearSVC(random_state=42, tol=0.05)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8132495370951286 & Balanced Accuracy:0.8072063253684082\n",
    "* `MLPClassifier(random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8552265578078104 & Balanced Accuracy:0.8481423086810057\n",
    "* `RandomForestClassifier(n_jobs=6, random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.7998545059385732 & Balanced Accuracy:0.7849452081564534\n",
    "* `BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), n_jobs=6, random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.851510234473268 & Balanced Accuracy:0.8448135971300434\n",
    "* `BaggingClassifier(base_estimator=KNeighborsClassifier(), random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8040109139580689 & Balanced Accuracy:0.797505718022774\n",
    "* `BaggingClassifier(base_estimator=GaussianNB(), n_jobs=6, random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.6889119940989887 & Balanced Accuracy:0.669408426557572\n",
    "* `BaggingClassifier(base_estimator=LinearSVC(random_state=42, tol=0.05), n_jobs=4, random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8390275836624488 & Balanced Accuracy:0.8352138257160011\n",
    "* `BaggingClassifier(base_estimator=MLPClassifier(hidden_layer_sizes=20, random_state=42), random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8578569784546044 & Balanced Accuracy:0.8518975756890494\n",
    "* `BaggingClassifier(base_estimator=RandomForestClassifier(n_jobs=6, random_state=42), random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.7968607680608969 & Balanced Accuracy:0.7796641415284126\n",
    "* `GradientBoostingClassifier(random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8185840971893368 & Balanced Accuracy:0.8075612858804577\n",
    "* `BaggingClassifier(base_estimator=GradientBoostingClassifier(random_state=42), n_jobs=4, random_state=42)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8203764375551932 & Balanced Accuracy:0.8083381914829314\n",
    "\n",
    "\n",
    "### **Results of attempted final (complex) classifiers (10 fold cross-validation)**\n",
    "* `StackingClassifier(estimators=[('bagging_logistic', BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), max_features=0.7, max_samples=0.85, n_estimators=25, n_jobs=6, random_state=42)), ('bagging_sgd', BaggingClassifier(base_estimator=SGDClassifier(random_state=42), max_features=0.9, max_samples=0.5, n_estimators=35, n_jobs=4, random_state=42)), ('bagging_mlp', BaggingClassifier(base_estimator=MLPClassifier(hidden_layer_sizes=5, random_state=42), n_estimators=3, n_jobs=1, random_state=42)), ('grad_boosting', GradientBoostingClassifier(learning_rate=0.2, n_estimators=113))], final_estimator=GaussianNB(), n_jobs=1)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8539910263036601 & Balanced Accuracy:0.8499916515400712\n",
    "\n",
    "\n",
    "* `VotingClassifier(estimators=[('bagging_logistic', BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), max_features=0.7, max_samples=0.85, n_estimators=25, n_jobs=6, random_state=42)), ('svc', LinearSVC(random_state=42, tol=0.05)), ('grad_boosting', GradientBoostingClassifier(learning_rate=0.2, n_estimators=113)), ('mlp', MLPClassifier(hidden_layer_sizes=200, random_state=42))])`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8594937452161666 & Balanced Accuracy:0.8574680661770182\n",
    "\n",
    "* `StackingClassifier(estimators=[('bagging_logistic', BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), max_features=0.7, n_estimators=15, n_jobs=6, random_state=42)), ('mlp', MLPClassifier(hidden_layer_sizes=113, random_state=42))], final_estimator=LogisticRegression(random_state=42), n_jobs=1)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8567770768849808 & Balanced Accuracy:0.8503984450908069\n",
    "* `StackingClassifier(estimators=[('bagging_logistic', BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), max_features=0.7, n_estimators=15, n_jobs=6, random_state=42)), ('svc', LinearSVC(random_state=42, tol=0.05)), ('grad_boosting', GradientBoostingClassifier(learning_rate=0.25, n_estimators=113)), ('mlp', MLPClassifier(hidden_layer_sizes=113, random_state=42))], final_estimator=GaussianNB(), n_jobs=1)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8567441188433959 & Balanced Accuracy:0.8528436948615201\n",
    "\n",
    "* `StackingClassifier(estimators=[('bagging_logistic', BaggingClassifier(base_estimator=LogisticRegression(random_state=42, solver='sag', tol=0.05), max_features=0.7, max_samples=0.85, n_estimators=25, n_jobs=6, random_state=42)), ('svc', LinearSVC(random_state=42, tol=0.05)), ('random_forest', RandomForestClassifier(n_estimators=200, n_jobs=6, random_state=42))], final_estimator=GaussianNB(), n_jobs=1)`\n",
    "\n",
    "  Classifier:\n",
    "\n",
    "  F1 Weighted-Score:0.8492895967124776 & Balanced Accuracy:0.8444886288625206\n",
    "\n",
    "\n",
    "### Results of the final classifier (10-fold cross-validation)\n",
    "Model: `VotingClassifier(estimators=[('bagging_logistic',BaggingClassifier(base_estimator=LogisticRegression(random_state=42,solver='sag',tol=0.05),max_features=0.7,max_samples=0.85,n_estimators=25,n_jobs=6,random_state=42)),('svc',LinearSVC(random_state=42,tol=0.05)),('grad_boosting',GradientBoostingClassifier(learning_rate=0.2,n_estimators=113,random_state=42)),('mlp',MLPClassifier(hidden_layer_sizes=200,random_state=42))])`\n",
    "\n",
    "\n",
    "\n",
    "Metrics (10-fold):\n",
    "Classifier:\n",
    "F1 Weighted-Score:0.8575316429138551 & Balanced Accuracy:0.8550719461773506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQEFCmbcXMmb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQPgm_ubXMmc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN CODE HERE\n",
    "cls = VotingClassifier(estimators=[('bagging_logistic',\n",
    "                              BaggingClassifier(base_estimator=LogisticRegression(random_state=RANDOM_STATE,\n",
    "                                                                                  solver='sag',\n",
    "                                                                                  tol=0.05),\n",
    "                                                max_features=0.7,\n",
    "                                                max_samples=0.85,\n",
    "                                                n_estimators=25, n_jobs=6,\n",
    "                                                random_state=RANDOM_STATE)),\n",
    "                             ('svc', LinearSVC(random_state=RANDOM_STATE, tol=0.05)),\n",
    "                             ('grad_boosting',\n",
    "                              GradientBoostingClassifier(learning_rate=0.2,\n",
    "                                                         n_estimators=113,\n",
    "                                                         random_state=RANDOM_STATE)),\n",
    "                             ('mlp',\n",
    "                              MLPClassifier(hidden_layer_sizes=200,\n",
    "                                            random_state=RANDOM_STATE))])\n",
    "cls.fit(X,y)\n",
    "#END CODE HERE\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
    "predictions = cls.predict(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnAp-d2DXMmf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER\n",
    "***\n",
    "\n",
    "The same problem as with 2.1 is observed: With each time the notebook is run the resulting accuracies change.\n",
    "\n",
    "As before, I report my last findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Neagvu0TXMmg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7K7iI7BXMmg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "  final_test_set = pd.read_csv('test_set.csv')\n",
    "  ground_truth = final_test_set['CLASS']\n",
    "  print(\"Balanced Accuracy: {}\".format(balanced_accuracy_score(predictions, ground_truth)))\n",
    "  print(\"F1 Weighted-Score: {}\".format(f1_score(predictions, ground_truth, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both should aim above 85%!"
   ],
   "metadata": {
    "id": "YJH-9KdOzW7z",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "EnsembleMethods.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}